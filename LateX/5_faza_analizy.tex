\section{Faza analizy}
\subsection{Wstęp}

\indent Block

\clearpage

\subsection{Cele i założenia}


{\bf Cel główny:}

Celem głównym niniejszej pracy magisterskiej jest zaprojektowanie i implementacja aplikacji webowej, która wykorzystując modele językowe dużej skali (LLM), będzie w stanie generować plany działania prowadzące do osiągnięcia określonych celów, takich jak przebiegnięcie maratonu czy nauka programowania. Aplikacja ma wspierać użytkowników w realizacji tych celów poprzez dostarczanie zadań i harmonogramów na kilka tygodni w przód.
\\

\noindent{\bf Cele szczegółowe: }
\begin{enumerate}
    \item {\bf Analiza i porównanie różnych modeli LLM:}
        \begin{itemize}
            \item[*] Przeprowadzenie badań nad dostępnymi modelami LLM, takimi jak CHAT GPT, LLAMA, Zephyr Stable, i ich zdolnościami do generowania zadań.
            \item[*] Porównanie wybranych modeli pod kątem jakości generowanych planów, efektywności oraz zasobów obliczeniowych.
       \end{itemize}
    
    \item {\bf Projekt i implementacja aplikacji webowej:}
        \begin{itemize}
            \item[*] Stworzenie interfejsu użytkownika za pomocą React, umożliwiającego łatwą interakcję z aplikacją.
            \item[*] Implementacja backendu w języku C\# z wykorzystaniem frameworku ASP.NET Core, który będzie obsługiwał logikę biznesową oraz komunikację z modelami LLM.
       \end{itemize}
    
    \item {\bf Integracja z modelami LLM:}
        \begin{itemize}
            \item[*] Zintegrowanie wybranych modeli LLM z aplikacją w sposób umożliwiający dynamiczne generowanie planów i zadań.
            \item[*] Zapewnienie mechanizmów optymalizacji i skalowalności w celu obsługi wielu użytkowników jednocześnie.
       \end{itemize}
    
    \item {\bf Budowa architektury mikroserwisowej w chmurze Azure:}
        \begin{itemize}
            \item[*] Zdefiniowanie mikroserwisów odpowiadających za różne funkcjonalności aplikacji (np. zarządzanie użytkownikami, komunikacja z LLM, harmonogramowanie zadań).
            \item[*] Implementacja i wdrożenie mikroserwisów w środowisku chmurowym Azure z użyciem takich technologii jak Kubernetes, Azure KeyVault.
       \end{itemize}
    
    \item {\bf Testowanie i walidacja:}
        \begin{itemize}
            \item[*] Przeprowadzenie testów funkcjonalnych i wydajnościowych aplikacji oraz poszczególnych mikroserwisów.
       \end{itemize}
    
    \item {\bf Dokumentacja i raportowanie wyników:}
        \begin{itemize}
            \item[*] Opracowanie pełnej dokumentacji technicznej projektu, obejmującej zarówno kod źródłowy, jak i szczegółowe opisy architektury i integracji.
            \item[*] Sporządzenie raportu z wyników porównania modeli LLM oraz wydajności i skalowalności aplikacji.
       \end{itemize}

\end{enumerate}

\noindent{\bf Założenia }
\begin{enumerate}
    \item {\bf Skalowalność:}
        \begin{itemize}
            \item[*] Aplikacja musi być skalowalna i zdolna do obsługi dużej liczby jednoczesnych użytkowników bez znacznej utraty wydajności.
       \end{itemize}
    
    \item {\bf Interaktywność i intuicyjność:}
        \begin{itemize}
            \item[*] Interfejs użytkownika musi być prosty, intuicyjny i przyjazny, aby umożliwić łatwą interakcję z aplikacją nawet mniej zaawansowanym technicznie użytkownikom.
       \end{itemize}
    
    \item {\bf Bezpieczeństwo danych:}
        \begin{itemize}
            \item[*] Wszelkie dane użytkowników, w tym dane osobowe oraz informacje o celach i zadaniach, muszą być przechowywane i przetwarzane zgodnie z najlepszymi praktykami w zakresie bezpieczeństwa i ochrony danych.
       \end{itemize}
    
    \item {\bf Elastyczność modelu:}
        \begin{itemize}
            \item[*] Aplikacja powinna być elastyczna i pozwalać na łatwe dodawanie nowych modeli LLM oraz aktualizacje istniejących, w miarę jak nowe technologie będą dostępne.
       \end{itemize}
    
    \item {\bf Użycie technologii chmurowych:}
        \begin{itemize}
            \item[*] Cała infrastruktura aplikacji, w tym mikroserwisy, bazy danych oraz usługi AI, zostanie wdrożona w chmurze Azure, co zapewni wysoką dostępność, niezawodność oraz łatwość zarządzania.
       \end{itemize}
\end{enumerate}

\subsection{Podstawa teoretyczna}
\begin{enumerate}
    
    \item {\bf Model językowy} - Model językowy w dziedzinie informatyki i sztucznej inteligencji to system komputerowy zaprojektowany do rozumienia, interpretowania i generowania ludzkiego języka naturalnego. Opiera się on na algorytmach uczenia maszynowego, głównie uczenia głębokiego, które uczą się struktury, semantyki i kontekstu języka przez analizę i przetwarzanie ogromnych zbiorów tekstów.

    Takie modele składają się z wielowarstwowych sieci neuronowych, z których każda warstwa przetwarza różne aspekty języka, od rozpoznawania słów po interpretację znaczeń. Centralnym elementem jest algorytm "transformer", który efektywnie przetwarza długie sekwencje tekstu, zachowując kontekst i znaczenie.
    
    Modele językowe są trenowane na dużych zbiorach tekstowych, zawierających różnorodne formy języka. Proces treningu polega na dostosowywaniu parametrów sieci neuronowej, aby jak najlepiej odwzorować naturalne użycie języka. W wyniku tego treningu, modele te są zdolne do tworzenia nowych, koherentnych wypowiedzi, reagowania na zapytania i analizy języka na wysokim poziomie.
    \\
    \item {\bf Model GPT} - Modele GPT (Generative Pre-trained Transformer) to rodzaj zaawansowanych modeli językowych opracowanych przez firmę OpenAI wykorzystujących architekturę transformer w dziedzinie sztucznej inteligencji. Zostały one zaprojektowane do generowania tekstu, interpretacji języka naturalnego oraz wykonywania różnorodnych zadań związanych z przetwarzaniem języka. Ich główną cechą jest zdolność do generowania spójnego i kontekstualnie odpowiedniego tekstu na podstawie dostarczonych informacji wejściowych.

    GPT opiera się na technice uczenia głębokiego, gdzie modele są trenowane na ogromnych zbiorach tekstowych w celu nauki struktury języka, jego semantyki oraz różnorodnych kontekstów. Model GPT, wykorzystując architekturę transformer, skutecznie przetwarza i analizuje długie sekwencje tekstu, co pozwala na zachowanie złożonego kontekstu i generowanie koherentnych odpowiedzi.
    
    Kluczowym aspektem modeli GPT jest ich pre-trening, czyli wstępne szkolenie na szerokiej gamie danych tekstowych. Dzięki temu modele te rozwijają ogólną zdolność do rozumienia i generowania języka, którą następnie można dostosować do konkretnych zastosowań (tzw. fine-tuning).
    
    Modele GPT są wykorzystywane w różnych aplikacjach, od automatycznego generowania tekstu, przez tworzenie odpowiedzi w systemach dialogowych, po zaawansowane analizy językowe. Ich zdolność do generowania naturalnego, spójnego i kontekstowo odpowiedniego języka sprawia, że znajdują one zastosowanie w wielu dziedzinach, w tym w rozrywce, edukacji oraz biznesie.
    \\
     
 \end{enumerate}

\subsection{Prompt engineering}

\subsubsection{Opis modeli językowych}

\begin{enumerate}
 \item {\bf ChatGPT} - to specjalistyczna wersja modelu językowego opracowana przez firmę OpenAI, zaprojektowana głównie do prowadzenia konwersacji w formie tekstowej. Jest to aplikacja modelu GPT skoncentrowana na interakcjach dialogowych, zdolna do generowania naturalnie brzmiących, spójnych i kontekstualnie adekwatnych odpowiedzi w czasie rzeczywistym.

    Podstawą działania ChatGPT jest zaawansowany model językowy oparty na architekturze transformer, który został wytrenowany na ogromnych zbiorach danych tekstowych, w tym na dialogach i rozmowach. Dzięki temu ChatGPT wykazuje zdolność do zrozumienia zapytań w kontekście konwersacji i generowania płynnych, spójnych odpowiedzi.
    
    ChatGPT wyróżnia się zdolnością do utrzymania spójnego kontekstu rozmowy, co pozwala na prowadzenie dłuższych interakcji, które wydają się naturalne i są bardziej angażujące dla użytkownika. Może on odpowiadać na pytania, prowadzić dyskusje na różne tematy, oferować pomoc lub informacje, a także angażować się w bardziej twórcze zadania, takie jak pisanie opowiadań czy wierszy.
    
    Model ten jest często wykorzystywany w aplikacjach do obsługi klienta, asystentach cyfrowych, edukacji, a także jako narzędzie do interakcji i angażowania użytkowników na platformach internetowych. Zdolność ChatGPT do naturalnej interakcji językowej sprawia, że znajduje on zastosowanie w różnorodnych środowiskach, gdzie istotna jest zdolność do prowadzenia płynnej, ludzko brzmiącej konwersacji.
    \\
    \item {\bf LLAMA} - LLAMA (Lifelong Language Model Agent) to model językowy opracowany przez firmę Meta, zaprojektowany został do wydajnej pracy z mniejszą ilością danych niż jego konkurenci. Wykorzystuje on technikę zwaną retreningiem, dzięki której jest w stanie dostosować się i uczyć na podstawie nowych informacji przez cały okres swojego działania, co znacznie poprawia jego skuteczność w zadaniach NLP (Natural Language Processing). LLAMA jest także zoptymalizowanym modelem językowym pod kątem wielozadaniowości, dzięki czemu sprawnie radzi sobie z różnorodnymi zadaniami językowymi, zapewniając przy tym efektywność w użyciu zasobów obliczeniowych.
    \\
    \item {\bf Zephyr Stable} - Zephyr Stable LLM to narzędzie do przetwarzania języka naturalnego, które bazuje na modelach uczenia maszynowego. Jest skoncentrowane na stabilności i niezawodności w czasie długotrwałego działania. Jego architektura jest zaprojektowana tak, aby zapewniać równowagę między dokładnością odpowiedzi a zachowaniem spójności i adekwatności w trakcie interakcji z użytkownikiem. Model ten jest szczególnie przydatny w aplikacjach, które wymagają trwałej i efektywnej komunikacji maszyna-człowiek, jak chatboty czy asystenci głosowi, gdzie niezmienna jakość i precyzja są kluczowe.
    \\

\end{enumerate}

\subsubsection{Analiza przypdaków użycia modeli językowych}

Rozwój modeli językowych, szczególnie za sprawą zaawansowanych technologii opartych na sztucznej inteligencji, wprowadził nowe możliwości w dziedzinie przetwarzania języka naturalnego. Jednym z kluczowych obszarów wykorzystania tych modeli jest prompt engineering, czyli kształtowanie i dostosowywanie zapytań (promptów) w celu uzyskania pożądanych odpowiedzi od modeli językowych. Niniejszy rozdział skupia się na analizie konkretnych przypadków użycia modeli językowych w kontekście prompt engineeringu.

Przed przystąpieniem do analizy przypadków użycia istotne jest określenie modeli językowych, które będą poddane ocenie. W ramach niniejszej pracy magisterskiej skupimy się na modelach opartych na architekturze GPT (Generative Pre-trained Transformer), a w szczególności na, GPT-3.5,GPT-4. Wybór tych modeli wynika z ich doskonałej zdolności do zrozumienia kontekstu, elastyczności w generowaniu tekstów oraz szerokiej gamy zastosowań w dziedzinie języka naturalnego.
 
Analiza przypadków użycia rozpocznie się od szczegółowego zrozumienia procesu prompt engineeringu. Proces ten obejmuje identyfikację celu zapytania, dostosowanie struktury promptu, a także optymalizację parametrów modelu w celu uzyskania precyzyjnych i zadowalających wyników. Przeanalizujemy różne strategie prompt engineeringu, w tym zmiany w sformułowaniu pytań, manipulacje długością promptu oraz eksperymenty z parametrami kontekstowymi.

\subsubsection{Projektowanie promptów}

\noindent\textbf{Wprowadzenie}

Projektowanie prompt'ów jest kluczowym aspektem w tworzeniu efektywnych interakcji z modelami językowymi. Precyzyjne i dobrze sformułowane pytania mają decydujący wpływ na jakość generowanych odpowiedzi. W tym rozdziale skoncentrujemy się na procesie projektowania prompt'ów, w tym na strategiach wyboru słów kluczowych, formatowaniu zapytań oraz eksperymentach z różnymi rodzajami promptów.
\\

\noindent\textbf{Analiza celu naszego prompta}

Każdy prompt, musi zostac tak zaprojektowany aby spełniał wymagania techniczne aplikacji.
Aplikacja jest podzielona na wiele etapów gdzie użytkownik otrzymuje specjalnie wygenerowane odpowiedzi w zależności od opcji, które wcześniej wybierze.
\\

\noindent\textbf{Wybór Słów Kluczowych}

Pierwszym etapem projektowania prompt'ów jest staranne dobranie słów kluczowych. Te słowa stanowią istotny element, który kieruje modelem językowym w odpowiednim kierunku. Analiza semantyczna i kontekstualna danego zadania jest kluczowa w identyfikacji słów, które mają kluczowe znaczenie dla uzyskania precyzyjnych odpowiedzi. W tym kontekście eksperymenty z różnymi formułowaniami pytań mogą prowadzić do odkrycia najbardziej efektywnych kombinacji słów kluczowych.
\\

\noindent\textbf{Struktura Prompt'ów}

Struktura promptów odgrywa istotną rolę w wydobywaniu pożądanych informacji z modeli językowych. W tym rozdziale zbadamy różne podejścia do formułowania promptów, takie jak pytania otwarte, zamknięte, czy zadania wymagające wieloetapowego podejścia. Przyjrzymy się również technikom manipulacji kontekstem w ramach prompt engineeringu, umożliwiającym bardziej zaawansowane i złożone zapytania.

\subsubsection{Prompt security}

{\bf Czym jest bezpieczeństwo promptów?}

\noindent Bezpieczeństwo promptów dotyczy ochrony przed złośliwym wykorzystaniem duzych modeli językowych (LLM) poprzez manipulowanie promptami, czyli wejściami dostarczanymi modelowi. Ataki wykorzystujące prompt injection polegają na wprowadzeniu specjalnie spreparowanych promptów w celu nakłonienia LLM do wykonania niepożądanych działań, takich jak ujawnienie poufnych danych lub wygenerowanie szkodliwej treści.
\\

\noindent {\bf Rodzaje ataków związane z prompt security:}

\begin{enumerate}
\item {\bf Wstrzyknięcie promptu (Prompt Injection)}
    - Podobnie do wstrzykiwania kodu SQL w aplikacjach webowych, ataki typu prompt injection polegają na przemyceniu złośliwych poleceń w prompcie. Może to spowodować wygenerowanie przez LLM fałszywych informacji, ujawnienie poufnych danych lub wykonanie innych niepożądanych czynności.
\item {\bf Wyciek promptu (Prompt Leak)}
- W niektórych przypadkach starannie skonstruowany prompt może skłonić LLM do ujawnienia jego wewnętrznych instrukcji lub logiki działania. Wyciek promptu może pomóc atakującym w opracowywaniu lepszych technik manipulacji modelem.
\end{enumerate}

{\bf Znaczenie bezpieczeństwa promptów}

\noindent Bezpieczeństwo promptów jest kluczowe dla zapewnienia odpowiedzialnego rozwoju i wdrażania technologii GenAI. Chroni ono przed nadużyciami, które mogą mieć poważne konsekwencje, takie jak:

\begin{enumerate}
    \item {\bf Utrata danych poufnych}
    - Wstrzyknięcie promptu może zmusić LLM do ujawnienia poufnych informacji, takich jak dane finansowe lub dane osobowe.
    \item {\bf Dezinformacja i propaganda}
    - Ataki mogą być wykorzystywane do generowania fałszywych wiadomości lub propagandy, która może wpływać na opinię publiczną.
    \item {\bf Spam i phishing}
    - Generatywne AI mogą być wykorzystywane do tworzenia masowych wiadomości spamowych lub realistycznych wiadomości phishingowych służących wyłudzaniu informacji.
\end{enumerate}


{\bf Rozwiązania bezpieczeństwa promptów}

\noindent Firmy zajmujące się bezpieczeństwem cybernetycznym opracowują narzędzia i techniki wykrywania i zapobiegania atakom na prompt security. Niektóre z rozwiązań obejmują:

\begin{enumerate}
    \item {\bf Analizę i monitorowanie promptów} - Wykrywanie podejrzanych wzorców w promptach wprowadzanych do LLM.
    \item {\bf Weryfikację danych wyjściowych} - Sprawdzanie, czy wygenerowane przez LLM dane są zgodne z oczekiwaniami i nie zawierają szkodliwych informacji.
    \item {\bf Szkolenie LLM w zakresie bezpieczeństwa} - Opracowywanie modeli odpornych na manipulacje poprzez odpowiednie ich szkolenie.
\end{enumerate}

{\bf Wnioski}

\noindent Bezpieczeństwo promptów to ważny, ale często pomijany aspekt rozwoju i wdrażania generatywnej sztucznej inteligencji. Wraz z ciągłym rozwojem technologii GenAI, konieczne jest opracowanie solidnych mechanizmów bezpieczeństwa w celu ochrony przed potencjalnymi zagrożeniami. Rozwiązania te zapewnią, że GenAI będzie wykorzystywana w sposób odpowiedzialny i przyniesie korzyści społeczeństwu.

\subsubsection{Fine-tuning}
Jest to modyfikowanie istniejących modeli językowych w celu dostosowania ich do konkretnego zadania lub zbioru danych, odgrywa kluczową rolę w dziedzinie uczenia maszynowego i przetwarzania języka naturalnego. Proces ten polega na dalszym trenowaniu modeli na specyficznych danych lub zadaniach, co pozwala na poprawę ich wydajności i skuteczności w konkretnych zastosowaniach. Fine-tuning modeli LLM może być stosowany do różnorodnych zadań, takich jak tłumaczenie maszynowe, generowanie tekstu, analiza sentymentu czy wykrywanie fraz kluczowych. W ten sposób możliwe jest dostosowanie ogólnych modeli językowych do specyficznych potrzeb użytkowników i zastosowań, co przyczynia się do poprawy jakości rezultatów oraz dostosowania modeli do konkretnego kontekstu biznesowego lub naukowego.

\subsubsection{Halucynacje sztucznej inteligencji}
Halucynacje sztucznej inteligencji to zjawisko, w którym modele językowe generują odpowiedzi, które nie mają podstaw w rzeczywistych danych, na których zostały przeszkolone.

Halucynacje to potencjalnie nieprawidłowe, niezamierzone i nierealistyczne wyniki generowane przez modele językowe, wynikające z ich niezdolności do zrozumienia istotnych informacji opartych na danych treningowych. Mogą one przybierać formę odpowiedzi, które brzmią przekonująco, ale nie mają pokrycia w faktach.

Głównymi przyczynami halucynacji mogą być ograniczone dane treningowe. Jeśli zestaw zbiorów danych jest niewystarczający lub brakuje różnorodności, model może mieć trudności z uchwyceniem złożoności języka, co prowadzi do tak zwanych halucynacji.

Kolejnym ważnym aspektem mogą byc, błędy w danych treningowych, jeśli zawierają luki lub błędne informacje modele mogą włączyć i wzmocnić te uprzedzenia, powodując halucynacje utrwalające fałszywe informacje.

Również dużą rolę w jakości informacji odgrywa szum danych wejściowych. Niekompletne lub zaszumione dane wejściowe utrudniają modelom dokładne przewidywanie, co prowadzi do zwrócenia błędnych odpowiedzi przez model językowy.

Halucynacje mogą prowadzić do generowania fałszywych informacji, takich jak sfabrykowane fakty, nieprawdziwe twierdzenia czy zmyślone sytuacje. Stanowi to poważne zagrożenie dla wiarygodności i dokładności informacji dostarczanych przez modele AI, szczególnie w dziedzinach wymagających wieloetapowego rozumowania.

Badania wykazały, że niektóre popularne chatboty, takie jak ChatGPT, Bard i Bing, mogą halucynować nawet w co czwartej odpowiedzi. Dlatego kluczowe jest świadome korzystanie z tych narzędzi i weryfikowanie uzyskanych informacji.

Firmy takie jak OpenAI podejmują wysiłki, aby zminimalizować halucynacje, stosując techniki takie jak "nadzór procesu" - monitorowanie procesu generowania odpowiedzi w celu wykrycia i skorygowania potencjalnych halucynacji.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{Obrazy/chatgpt_trick.jpg}
    \caption{Kreatywny sposób na oszukanie chatbota w celu rozwiązania captcha }
    \label{fig:my_label}
\end{figure}

\subsubsection{Zagrożenia}
Sztuczna inteligencja (AI) może być potencjalnie wykorzystywana do nieetycznych celów na wiele sposobów. 

Jeśli dane treningowe dla modeli AI zawierają uprzedzenia lub wzorce dyskryminacji, algorytmy mogą je utrwalać i skalować na dużą skalę. Może to prowadzić do niesprawiedliwych decyzji i nierównego traktowania niektórych grup społecznych. Zaawansowane systemy AI mogą być wykorzystywane do masowej inwigilacji, śledzenia i gromadzenia danych osobowych bez zgody i wiedzy osób, naruszając ich prawo do prywatności.

Modele językowe mogą być użyte do tworzenia fałszywych treści, takich jak deepfake'i w postaci fałszywych nagrań, sfabrykowanych wiadomości.

Nieodpowiednio wykorzystane zdolności sztucznej inteligencji mogą być wykorzystywane przez hakerów do tworzenia zaawansowanych wirusów, botnetów i innych form szkodliwego oprogramowania, które są trudniejsze do wykrycia i zwalczania.

\subsubsection{Large Language Models}
Duże modele językowe (LLM) to zaawansowane algorytmy sztucznej inteligencji, które wykorzystują techniki głębokiego uczenia się i obszerne zbiory danych do generowania, podsumowywania i przewidywania nowych treści w języku naturalnym. Modele te, opierają się głównie na transformatorach, są zaprojektowane tak, aby rozumieć relacje między słowami i frazami, umożliwiając im równoległe przetwarzanie całych sekwencji tekstu. Szkolenie odbywa się na ogromnych ilościach danych, zazwyczaj z miliardami parametrów, co pozwala im na dostarczanie dokładnych i szybkich odpowiedzi. Modele językowe wiążą się również z wyzwaniami, takimi jak wysokie koszty rozwoju i operacyjne, potencjalna stronniczość danych szkoleniowych, brak możliwości wyjaśnienia wyników, ryzyko halucynacji (dostarczanie niedokładnych odpowiedzi) oraz złożoność ze względu na dużą liczbę parametrów.

\subsubsection{Ranking Modeli}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{Obrazy/llms_benchmark.png}
    \caption{Benchmark modeli LLM}
    \label{fig:my_label}
\end{figure}


\subsubsection{Etyka}
Wraz z bardzo szybkim rozwojem sztucznej inteligencji, pojawia się wiele dylematów natury etycznej związanych zarówno z prawami autorskimi trenowanych danych oraz faktycznych rezultatów stworzonych przez AI. Istnieje ryzyko, że modele mogą przekazywać błędne informacje, szerzyć dezinformację, lub być źródłem treści szkodliwych często nielegalnych. Dlatego ważne jest, aby organizacje i twórcy tych modeli dbali o uczciwość, transparentność, oraz brali odpowiedzialność za generowane treści.

\clearpage

\subsection{Platformy chmurowe}
Platformy chmurowe to zaawansowane systemy, które umożliwiają firmom i organizacjom dostęp do zasobów obliczeniowych przez Internet, bez konieczności posiadania i zarządzania własną infrastrukturą IT.

Platforma chmurowa to komponenty składające się z systemów operacyjnych, serwerów w centrum danych skonfigurowanych pod personalne potrzeby klientów. Umożliwiają one organizacjom wynajmowanie dostępu do zasobów obliczeniowych, takich jak serwery, bazy danych, magazyny danych, analityk, sieci wirtualnych. Model biznesowy dostawców usług chmurowych opiera się na zasadzie "pay as you go" (z .ang) płać za te zasoby, które zużyjesz.

Usługi chmurowe możemy podzielić na trzy kluczowe rodzaje:

1. Chmura Publiczna: Usługi świadczone przez zewnętrznych dostawców, dostępne dla wielu klientów przez Internet. Przykłady to Amazon Web Services (AWS), Google Cloud Platform, Microsoft Azure, Alibaba Cloud i IBM Cloud.
2. Chmura Prywatna: Dedykowana dla jednej organizacji, może być zarządzana wewnętrznie lub przez zewnętrznego dostawcę. Oferuje większą kontrolę nad zasobami i bezpieczeństwem.
3. Chmura Hybrydowa: Kombinacja chmury publicznej i prywatnej, umożliwiająca przenoszenie danych i aplikacji między nimi, co zapewnia większą elastyczność i optymalizację infrastruktury.

Korzyści z Używania Platform Chmurowych:

1. Elastyczność: Możliwość szybkiego skalowania zasobów w górę lub w dół w zależności od potrzeb biznesowych, co pozwala uniknąć nadmiernego lub niedostatecznego przydziału zasobów obliczeniowych.
2. Redukcja Kosztów: Eliminacja kosztów kapitałowych związanych z budową i utrzymaniem własnych centrów danych oraz płatność tylko za faktycznie używane zasoby.
3. Wydajność: Dostęp do dużej mocy obliczeniowej i magazynowej na żądanie, co pozwala unikać wąskich gardeł i zapewnia lepszą wydajność aplikacji.
4. Szybkość Wdrożenia: Możliwość szybkiego wdrażania technologii na całym świecie, co skraca czas wprowadzenia produktów na rynek.
5. Zwiększona Produktywność: Zespoły IT są zwolnione z zarządzania, utrzymania i aktualizacji sprzętu i oprogramowania na miejscu, co pozwala im skupić się na bardziej strategicznych zadaniach.
6. Bezpieczeństwo: Dostawcy chmurowi inwestują znaczne środki w technologie zabezpieczające, co często zapewnia wyższy poziom bezpieczeństwa niż w przypadku własnych centrów danych.
7. Niezawodność: Platformy chmurowe są bardziej odporne dzięki rozproszonej infrastrukturze, co zapewnia szybsze odzyskiwanie danych po awariach systemów.

Platformy chmurowe odgrywają kluczową rolę w nowoczesnych strategiach IT, umożliwiając firmom szybkie i efektywne skalowanie swoich operacji oraz wprowadzanie innowacji.
\subsubsection{Opis platform}
Platformy chmurowe to zintegrowane zestawy technologii umożliwiające użytkownikom zdalne korzystanie z szerokiej gamy zasobów komputerowych przez Internet. Te cyfrowe ekosystemy oferują skalowalne rozwiązania IT, takie jak serwery, pamięć, bazy danych oraz oprogramowanie, wszystko dostępne jako usługa. Główne kategorie tych usług to: Infrastructure as a Service (IaaS), Platform as a Service (PaaS) oraz Software as a Service (SaaS).

Korzystanie z platform chmurowych przynosi wiele korzyści. Elastyczność i skalowalność pozwalają na szybkie dostosowywanie zasobów do aktualnych potrzeb, bez konieczności inwestycji w drogie sprzęty i infrastrukturę. Oszczędności kosztów wynikają z modelu opłat "pay as you go", co oznacza, że płaci się tylko za te zasoby, które są faktycznie wykorzystywane. Bezpieczeństwo danych jest również kluczowym aspektem, z zaawansowanymi rozwiązaniami chroniącymi przed utratą danych i cyberatakami.
\subsubsection{Wybór - Plaforma Microsoft Azure}
\clearpage
Platfroma Azure została wybrana do naszego projektu ze względu na to,że nasz zespół miał już doświadczenie zarówno prywatne jak i komercyjne w tej technologi. Kolejnym atutem rozwiązania firmy z Redmond, jest bogata oferta serwisów oraz technologii potrzebnych do zbudowania do aplikacji opartej o architekturę mikroserwisów.
W tym kontekście, Azure okazuje się być idealną platformą do rozwijania mikroserwisów dzięki usługom takim jak Azure Kubernetes Service, który upraszcza zarządzanie kontenerami, oraz Azure Container Instances. Platforma oferuje również integracje z narzędziami deweloperskimi oraz CI/CD.

\subsection{Wymagania funkcjonalne}

\subsubsection{Opis funkcjonalności systemu}

\subsubsection{Diagram przypadków użycia}

\subsubsection{Scenariusze przypadków użycia}

\subsubsection{Sposób przechowywania danych}
Dane użytkowników są przechowywane w bazie danych PostgreSQL, co zapewnia efektywne zarządzanie dużymi zbiorami danych oraz wysoki poziom bezpieczeństwa. PostgreSQL, jako zaawansowany system zarządzania relacyjnymi bazami danych, oferuje szerokie możliwości skalowania i optymalizacji, co jest kluczowe przy obsłudze danych wrażliwych i osobowych.

\subsubsection{Diagramy}
\clearpage

\subsection{Wymagania niefunkcjonalne}
\begin{itemize}
    \item[*] W przypadku gdy użytkownik korzysta z komputera stacjonarnego lub urządzenia mobilnego aplikacja powinna działać na: Google Chrome(wersja 95 i wyżej), Firefox lub Opera. 
    \item[*] Aplikacja powinna poprawnie działać na środowisku chmurowym np. Azure. Rekomendowanym systemem operacyjnym jest 64 bitowy system GNU/Linux, ponieważ najlepiej jest on przystosowany do środowiska chmurowego. W przypadku użycia kontenerów zalecane jest utworzenie własnego repozytorium do przechowywania źródeł obrazów np. Azure Conainer Registry. Ddla każdego uruchomionego kontenera należy zapewnić minimum 1 rdzeń procesora oraz 2GB pamięci RAM. Dla optymalizacji działania aplikacji, wszystkie komponenty powinny się znajdować w tej samej lub bardzo do siebie zbliżonej lokalizacji, aby uniknąć zbędnych opóźnień.
    \item[*] Aplikacja uruchomiona na lokalnym środowisku powinna poprawnie działać na komputerze z zainstalowanym system operacyjnym wspierającym konteneryzację Docker oraz wirtualizację np. Ubuntu Linux lub Windows 10/11. Najlepiej używać procesora o architekturze "x86\_64/amd64" i posiadać minmalną ilość pamięci RAM wynoszącej 4GB.
\end{itemize}
\clearpage

\subsection{Opis prototypów}
Prototypy opracowane na etapie planowania całej aplikacji okazały się niezwykle pomocne w ustaleniu funkcjonalności oraz ogólnego zarysu aplikacji. Jednakże, po dokładnej analizie trendów dotyczących UX/UI oraz tematyki aplikacji, konieczne było wykonanie nowego aspektu wizualnego. Zmieniono kolorystykę oraz doprecyzowano wygląd poszczególnych komponentów, aby spełnić wymagania standardów UX/UI oraz zapewnić użytkownikom maksymalny komfort podczas korzystania z aplikacji. Ważnym celem było stworzenie projektu przyjaznego i intuicyjnego, a także spełnienie wymogów dotyczących dostępności. Wszystkie te aspekty zostały uwzględnione podczas projektowania i implementacji aplikacji.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{Obrazy/prototypy/logowanie.png}
    \caption{Prototyp ekranu logowania}
    \label{fig:my_label}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{Obrazy/prototypy/personalizacja_konta.png}
    \caption{Prototyp ekranu personalizacji konta}
    \label{fig:my_label}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{Obrazy/prototypy/strona_startowa.png}
    \caption{Prototyp strony startowej}
    \label{fig:my_label}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{Obrazy/prototypy/panel_zadania.png}
    \caption{Prototyp panelu zadania}
    \label{fig:my_label}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{Obrazy/prototypy/kalendarz.png}
    \caption{Prototyp kalendarza}
    \label{fig:my_label}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{Obrazy/prototypy/profil_uzytkownika.png}
    \caption{Prototyp profilu użytkownika}
    \label{fig:my_label}
\end{figure}

\clearpage